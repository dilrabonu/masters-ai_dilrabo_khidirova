{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMqNyPKuji6Iijtu2hI3eLJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nONdKpzUfqOH","executionInfo":{"status":"ok","timestamp":1734688693814,"user_tz":-300,"elapsed":26914,"user":{"displayName":"Dilrabo Khidirova","userId":"07641774209207942309"}},"outputId":"8e9fc491-8af3-4a3a-8440-09f85094595b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-4j2jmu7n\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-4j2jmu7n\n","  Resolved https://github.com/openai/whisper.git to commit 90db0de1896c23cbfaf0c58bc2d30665f709f170\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n","Collecting tiktoken (from openai-whisper==20240930)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Collecting triton>=2.0.0 (from openai-whisper==20240930)\n","  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n","Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803583 sha256=0c984fe34904a9d2c1696d4591280a73c8115c240441a15c3082274af339ea06\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-3n0wxssp/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n","Successfully built openai-whisper\n","Installing collected packages: triton, tiktoken, openai-whisper\n","Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n"]}],"source":["!pip install git+https://github.com/openai/whisper.git\n"]},{"cell_type":"code","source":["import whisper\n","import os\n"],"metadata":{"id":"8oDSTe8mfxga","executionInfo":{"status":"ok","timestamp":1734688816495,"user_tz":-300,"elapsed":5144,"user":{"displayName":"Dilrabo Khidirova","userId":"07641774209207942309"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["model = whisper.load_model(\"small\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uSbvTrtfgmUm","executionInfo":{"status":"ok","timestamp":1734688847692,"user_tz":-300,"elapsed":28171,"user":{"displayName":"Dilrabo Khidirova","userId":"07641774209207942309"}},"outputId":"7dcb6c89-1a85-4330-abcd-72075a990cc6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|███████████████████████████████████████| 461M/461M [00:19<00:00, 25.4MiB/s]\n","/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(fp, map_location=device)\n"]}]},{"cell_type":"code","source":["audio_path = \"/content/ITPU_MS_Degree_Session_5_-_Generative_AI-20241213_153714-Meeting_Recording.mp3\"\n","\n","# Transcribe the audio\n","result = model.transcribe(audio_path)\n","\n","# Print the transcription\n","print(\"Transcription:\")\n","print(result[\"text\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LvdHF5qBgscC","executionInfo":{"status":"ok","timestamp":1734688987133,"user_tz":-300,"elapsed":134474,"user":{"displayName":"Dilrabo Khidirova","userId":"07641774209207942309"}},"outputId":"35c00eaa-9c83-4d91-8a5a-322ef7681ca3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Transcription:\n"," Good evening guys. Good evening. Hello. Did you choose Mr. Degis Alt? Not yet. I don't know. I chose it. Send it out. Maybe two, 30 minutes ago. Today, too. Make sure. Today is a deadline, yeah? It's yesterday. By the way, do we have strict deadlines for homework, guys? No. Isn't it that strict? Because I'm not sure. I did. Yeah. It works by the. But I would suggest to find at least 10 to 15 minutes every day how I am doing. Because when it will come at the end, it will be like everything. So at least I'm just trying to find 10, 15, 20 minutes doing like by small, small, small parts. Then every day. I'm from the course. From others. Any, I think. Other courses, I think they didn't give you homework yet. Yeah. They gave it. They gave it. Data. Data processing they gave. It gave. He gave yesterday, I think. This file format. I missed the class yesterday. I'll watch it today. Hmm. I guess. Hello, Italy. Hello, Italy. What are you doing? I have a question. Wow. Yeah, I'm not feature. If you see, so two days ago, Google introduced quantum computers. Yeah. Our second lesson, you said that there is a problem. There is a problem with the models because they have a resource problem. And what will be in the future about it? Because if Google, if we have, they have a quantum computers, then they have a huge resource. And how they will change. Yeah. So that's really good. That's interesting question. So I didn't know how they work like only very general concept of these QBs. I still, you know, I've been studying quantum physics in university, like at least one year, probably two years, I can't remember. You know, a lot of electronics works on quantum effects. So for example, some, some stuff allows the electrons to jump the barrier. They like actually cannot jump. So they like tunneling through that. And no one knows like how it works, but it works. And we use it in TVs and smartphones. And that's it. Like some, some kind of some kind of magic. So it's still, I think there should be some kind of scientific explanation, but I didn't know like how these quantum computers work. I don't understand the concept of like something having the value of zero and one at the same time, because like, okay, you can imagine that, but how about the storage? Right. So if you talk about like HDD or SSD, any storage, so you need to write it. And you need to like, you should be able to read it in the future. And the most fear, like two fears I remember, like from the community regarding quantum computers is, okay, if they are so powerful and can make like much more like, I don't know, millions times computations per second. So what about the encryption, right? So RSA or any other encryption algorithm. So basically encryption works, like modern encryption works on the very basic example, like there are how it's called like one way function in maths. I don't know how it's in English. So the way like, it's very easy to calculate this, for example, it's very easy to multiply to prime integers and get the result, but it's super hard to understand like what are the integers you multiply it if you receive like this result. And a lot of encryption is based on that and actually Bitcoin is based on that. So Bitcoin has several, like Bitcoin has a lot of protocols inside, like technologies and most of them like top tier encryption. We have right now. So if Google can beat that, so they can possibly like hijack all the new Bitcoins. Maybe they can rewrite the whole Bitcoin, like the whole blockchain, like in a day and I don't know, like steal all the Bitcoins from us. So a lot of like military applications like all this encryption and networking. So I don't know, it's technically possible that such technology can appear, but taking an account how much time and money and resources and smart people it takes to build such thing. So maybe the first country which builds it, then you know, like, who creates some kind of like snow crash, like from cyberpunk books, I don't know. So we'll see. At least like, I know like from what I understand like IT history and technology history, like all the new like top tier technologies, they have like two applications like the two industries, basically driving them fast in like making them broadly available is porn industry and military. So this is like two things usually picking up the latest stage and trying to get like money of debt. And maybe one of them will pick it up. I don't know, maybe military first. So I'm gonna see the new applications of quantum stuff. Yeah, but from personal perspective, like being a human like, you know, like IT guys. So yeah, that's interesting. But unless I cannot benefit from that, I cannot, unless I cannot like earn money doing that. So it's inevitable for me like the sun, the sun just can stop. And I like, okay, bad example, because the earth rotates across the sun. So the earth can just stop rotating and it will be infinite night for me. So I can't do anything about that. I can't do anything about rain. So what I can is just to buy the umbrella. I didn't know parade during stops sometimes. And in case I have the problem or some something I cannot influence. I think it's good not to worry about it. Because if you worry about something you cannot change, that's bad. So you spent a lot of resources and the result will be nothing like for sure. So my plan is just read the news. I don't know, follow this agenda, but don't worry about it. Okay. Yeah, let's, let's get down to business. So let me try to open my, on our schedule. I have to reboot my PC today. So I lost this ex was precious. Okay, this one. Okay, so the good news. Good news. We are on track, right? And here, here we are, meeting number five. So today we're going to discuss whisper API and local installation. The bad news. I took a look at some of the Chrome works, like two or three, maybe five, but not like very, very high level. I like the blog posts you guys made. So I like the pictures and I think I will share some of my favorite pictures next time. So I assume I will need this weekend to check them and probably to add in Alex at some marks or something like that. So just give me some time, but I haven't found any like critical problems there. So just remember to, to share the workflow, how you, how you did that, like for the first task. So I need to see like what prompts you've been using and don't forget about the pictures. So the pictures are necessary for our blog post. So I think we're going to focus on, on that probably either on this lecture or this one. We're not have time to review the homework. So today we're going to discuss the whisper API and this is very, let me show the slides. So here, here are the slides for today. So what is the whisper and why we're discussing that? So mostly when people discuss the generative for AI or AI in general, so a focused on like LLMs, I know like copilot's functions and stuff like that. So everything that works with text, right? Because it has more application to the business when you work with text. But still, usually what is being forgotten is images and audio processing. And regarding the images, I can't say this like easier to monetize this. So not too much business applications. So I know a lot of, I know a lot of cases where clients came to us like to the palm and asks to create something. Like, I don't know, maybe a rate of over a hundred of cases. And only, only several of them were related to image generation, but some of them. Like more, more of these cases are related to audio and one of the usual like request is okay guys, we have 5000 hours of recordings of, I don't know, meetings or customers calling us for the support. So we need to understand this data, right? So imagine you have a first line support like people calling, like real users calling describing the issue and the operators are just sharing. So what should you do to solve it? And you need to control it somehow, right? So you need to see what are the most popular questions or what are the answers. So is the client is happy or not, right? And for this case, you need to voice processing and processing the voice is actually very old technology. I don't remember how old it is, but like two things here usually working is text-to-speech and speech-to-text. And if we check, I don't know, like let's check Azure text-to-speech. Okay, it's a yeah, it's speech now, all right, I got it. Everything is a yeah now. Let me check the pricing actually. TTS pricing, it's super cheap, it's super cheap. Okay, I think here. So this is, okay, free tier, all right. There's a go, all right, delete this one. So speech-to-text. So in order to transcript something, like if you have a phone call, real-time transcription, $1 per hour, that's too much. Okay, batch transcription. So if I have a lot of audio calls, I didn't know like one hungry, I can post them as a batch and this will result me in 80 cents per hour. So not too much actually. Yeah, as usually like Microsoft has like very complicated, very complicated stuff for billing, so it's hard to understand how much you will spend. Yeah, and here is like the text-to-speech. So basically two directions like from speech, you can extract text and from text, you can generate speech, right? And this is like the standard voice. I think this standard voice become neural like during like last year because previously they had different pricing. Like the standard voice is like this robotized voice you usually hear when you call a bank or something. And neural voice is like more pleasant, more realistically, more naturally told. So right now I don't think they should have some kind of like cheap voice like this old cheap voice. Probably it's inside the Azure EA studio, but the truth is it's not expensive, right? So one billion characters, it's like it's a lot. $15 for business, it's like well, not too much I think. So and in case we have 1000 of like phone calls transcripted, so it will result as only like less than $200. So that's acceptable for that. And they definitely use machine learning there, right? So because there's no way programmatically like algorithmically to transcribe the speech. But still the interesting thing here is it's also possible to use machine learning techniques and all this concept of token we've been discussing previously, right? In working with speech in understanding the speech. And the project, the project I would like to show you this whisper. So let me find it should be here in the notes I put. Yeah, this one. It was introduced very similar once the child GPT arrived, right? So I'm just trying to recognize what was the time when charge it released. Okay. Okay. November 30. All right. 2022. Yeah, so they introduced the whisper like couple of months before charge it to know what happens like do you remember this movie? This one. So do you know this movie? It's kind of like popular from scientific perspective, but they got the problem right so this is the cyber like not cyberpunk but probably like interesting science fiction, but take a look at the release. The year is 1999. The problem is that in 1999, this movie gets out matrix. And that's why like everyone known about the matrix. It was like very successful movie. And this is the reason much more less people know that this movie for even exists. If they release it like a year previous before matrix so much more success or the same as a whisper right so charge it with your lease November 14 and whisper actually the same company open the eye but it was not so hyped. In September, as it become in November right. So what they introduced is they actually been working like at the same time looks like they've been working at the whisper and parallel like working with Shazer PC right so they reused the same, like not the same but similar architecture like encoders decoders and tokens to predict the tokens but not from text, but from audio as well. So they just take a look at the idea right and in the paper so they share like this one. So they took 680,000 hours of multi lingual audio. So they sliced it in 30 seconds time like steps, not steps but chunks and they, the train the model, the train the model against it using the same approach. They useful for the GPT. So pretty the same like slice the data into some some samples. Try to predict the tokens like compare with the result learn train the model and so on. So, and this results in a very interesting thing. Very high quality and I don't know if there are any model right now which can beat the quality of whisper because once it was released, it was like very high quality they released another model. A couple of months ago, I think it's called whisper but the architecture seems to stay to stay the same. So, and one more interesting thing here is, it's completely open sourced. So you can download it and it doesn't require a lot of VRAM to run so you can run it on the GPU, you can run it a bit slower on the CPU. And as far as it's like not large language model. It will perform like with acceptable, acceptable speed I've been running it at the CPU once and it works normally so not so slow as the LLM. So we are not so interested in technical details and I don't understand them actually just to explain to you. But what we're interested in is actually we interested in the result right so how it works and how I can use it. So, how to use it so you just install it as a Python package but you need by touch. And this is like why I mentioned the fighters previously so in order to install it locally. So, you must have the pytorch. And this is the command. It should be pytorch. Download probably now. Come on, where's download. Get started. Oh, here install. Okay, like if you would like to try this pair, right or in future if you would like to work with pytorch. So, there's like two options here. So it doesn't matter which version you will be installing you're going to see the same table for every time like it's couple of years the same. So, if you're installing it like in straightforward like recommended way, like usually it's windows, Python package or pip, Python, and here is like CUDA or CPU and depending on what you will select the URL will be different right. So CUDA is the accelerated computing framework created by NVIDIA as far as I remember by NVIDIA to run the computations on the GPU. And if you download and install this pytorch, you will be running everything against the GPU. But if you do not have GPU right so you can install it without like just the CPU. And it will be like the same pytorch it would work the same but much more slower right. But it will still work. So you need to you need to prior to install the pytorch. So first you need to Python. Next you need to select the CUDA version. So usually the later the better you just copy this and install. And actually I did that today because my pytorch, well not pytorch but whisper failed to run today. So I've been preparing to the demo and I also have like the some files, audio files. So, and the problem is so why facing them and you may face the same. So let me let me show you. So the problem with Python is that somehow like for for modern applications I use NumPy version two. So NumPy is a framework is a library to working like with numbers in Python, and it's required by pytorch. But this this pytorch and it's like implementation in whisper doesn't work with NumPy 2.0.2. So it requires like old version. And that's, that's kind of problem you may face as well. So what I did is actually I installed like virtual environments. So I created the virtual Python environment and downloaded once again pytorch, whisper and all the stuff. So, at least now it works right. So that's that's the problem you may face with working with any software in Python. At least I think there should be some kind of work around but taking a look at how many people using the virtual environments. I think this is this is the acceptable workaround. So getting back here so what you need is just to install the library and that's it you can install it from source but I prefer like this lazy option. And it also requires ffmpeg so ffmpeg. This this is the most popular library to working with audio and video so and actually this may be useful for you in future so I know like working in development or just processing media like audio and video requires you to convert files from one format to another split them. I don't know change the bitrate and anything so everything can be done with ffmpeg but ffmpeg is a command line interface like it's a program which allows you to do everything but without the user interface without the UI but you can you can work with it programmatically. So for example this is how you can convert from mp4 to AV like the old format right and this is like the the greatest software and the fastest solution right now I think so a lot of audio and video converting software use ffmpeg under the hood. So it will be necessary as well if you would like to start with whisper because whisper will need to transform the data from one format to another. So for example in my examples I'm gonna use mp3 but in some cases it may be Vav format or something like that so this will need to transform from one format to another. And here are the models and languages so as far as it's machine learning model right so you can pick up any model and getting back here so remember we've been discussing this like model card. Quantization distillation and so on so different models right so this is like 13 billion model this is 70 billion model the same with whisper. So this model has 39 million parameters it's tiny and it requires only one gigabyte of video RAM like VRAM and it's very fast right so assuming the large model has like 1.5 billion parameters it will occupy 10 gigabytes of VRAM and the speed is one like 1x right so this one is 10 times faster and turbo is 8 times faster. And that's it they have large V2 and large V3 and what's more interesting here is the quality so the quality here is measured in WER like warp error rate. So it just evaluating how many words are mispredicted like how many words are wrong and you can see the languages here so depending on the data set and probably the language structure this really differs right so I'm not surprised the Spanish is the has like less errors than any other languages. Because Spanish at least from my perspective I know like a couple of words and it's it looks like very easy language right like straightforward and kind of easy. Yeah and some of the languages probably lacking the presence in the Internet will result in like half of the words misspelled like Belarusian. I don't know I think I need to I know Belarusian language and I think I need to find some podcast in Belarusian and try to feed it into the whisper and check like how it works. But this this someone strange because 42 out of 100 comparing to six in Ukrainian so Belarusian Ukraine is super similar super similar so I can understand like 90% of Ukrainian and Ukrainians understand 90% of Belarusian probably is the problem in the data set right. Okay but in case like in most cases we're gonna work with English right so working with these languages are super cool. And command line users is pretty easy. So let's run the let's run the example and for the example I have the. I took the matrix movie and you know like there is a scene. Agents meet is talking with morpheus. Let me just matrix agents meet. Is he's delivering the speech about like the humans. I'm not sure you will be able to. To hear it because I'm using teams in in browser so it's not desktop application so I can't share. My audio but here is like the agents meet is interrogating morpheus and he's delivering this pitch like I got the idea so human beings are a virus and we like the agents are the cure so I will drop you the link or you can just Google it. And and reconsider this is I picked this sample because like high quality speech and know any like music in the ground and something like that. So let me try to run this and check how it will work. So default. Whisper. We just need to call whisper and pass the parameter as the data file. So right now is detecting the language isn't first 30 seconds and the language is detected to English. So you can do that actually manually you can specify the language and here is the output rate. So this is like the Smith is. Discussing it this is very precise. Actually this is very precise. I think it's like. Probably. Here we have the problems. Yeah, I think it's messing it's messing something still but I don't know what model I'm using so let me call the. The usage. Okay, so a lot of things here. So we are interested in the model. So we can specify as a model from here right. So these are the available models. I don't know which one is the default maybe small or base. So I have a lot of VRAM so I can actually launch launch large model. So I can actually launch large model. So let's try with probably another model and you can also specify the language explicitly. I don't think it will help us because it automatically recognize that this is English. One more thing actually very interesting thing. You can do with this pro. So here should be a parameter which is called prompt. You find it. Trace. Trace. Okay, model device output verbose. The should be parameter which is called something like a prompt. No. Yeah, this one initial prompt. So the trick is. It's not my default. It's none. But let's get back to the idea. So this pair is it. So this pair works with tokens right and everything like in this architecture you can you can pass the system message so you can pass the system message and whisper. And I've been working once on English transcription task. So we decided to make to transcribe the English assessments in our company. So there is like an interview asking questions and the person replying. And one of the idea was to calculate the, I don't know how it's called like the words like when the person is thinking like parasite words. So it's not possible to do with just plain whisper. Hardly possible but if you explicitly provide them in a prompt it will recognize them. And also a lot of a lot of times in this dialogue, they were mentioned to a company name E-POM and there is no such word in in whisper data set so that's why it was looking for a similar. But if you provide it in initial prompt and it will like see the same token combination. It will just do not search for any other alternative. So this is very powerful thing like very underrated. Okay, so let's get back here and let's try the model. Let me try different models and we're going to see the performance like this speed. Let's try with tiny the smallest model and let's check how fast it will work. Pretty fast. And let me try turbo. Okay, pretty fast as well. I don't see a big difference in speed here, right? At least at this sample. The sample is on the one minute. Okay, yeah, the final token was took the time and let me try large. So the large should give us the highest quality. Okay, yeah, you see the difference. So large and turbo versus the tiny. So the tiny you can just understand what's what's going on in this discussion. So if you don't care about particular worlds, if you just would like, I don't know, to summarize it in future and make some decision like, I don't know, sentiment analysis or something. So maybe tiny is okay. But still, that's a lot of questions as far as I see. But here the turbo and the large so they work very effectively. All right. So, and once I run every model, so it's getting downloaded. So, I don't think I started small. So probably it will be downloading right now. Now, now it's already downloaded. Yeah, so once I selected large first time, it started downloading all of this model like one and a half gigabytes of model locally. So if you have a lot of files, I didn't know, million of hours. So what you just need, you just need to PC. So you can use it in cloud, right? So let's check the pricing. Open the icon API pricing. So you can do that with API. Let me scroll to the whisper. Fine tuning, real time, systems. Okay, images, audio models. Okay. So what we're going to pay, we're going to pay 0.16 cents per minute. So let's let me calculate one hour. So, 0.6 cents per minute. So it means one hour will cost us 36 cents. Yeah, so it means like 100 hours of transcription will result in 36 dollars. So there should be some point where it's cheaper to purchase hardware and run it on your hardware and save the hardware for the next time. So save the hardware for yourself instead of sending to the to the open the right. Yeah, so in some cases it's more effective to do it on your own machine. I didn't know like spend the night create some Python scripts. And yeah, regarding Python, you can call it from Python as well. So it's Python library. So you can, you can use it from common line, but you can also, here is the common time usage. It can also import it, load the model and transcribe your own data just as passing is the reference to the to the file right and here are the examples how you can do that with with Python. Still, it has one problem, which probably being solved in some forks of the whisper like whisper accent so on. The problem is it doesn't differentiate speakers. So you will not see the difference between speaker one and speaker two right. So what I did in homework I shared the file with you where there is a difference between speaker one and speaker two so here is no no any difference but the output formers may be different right so let me call the help once again. Here output format you can see we have a lot of different output formers txt, tsrt, tsvgs on all. Let's try. Let's try to run this all but probably all is a default right so let me check. Okay, yeah, all is a default it means I already have all this information available so let's let's check it out. So you can run this. Let me format it. Very good, right.\n"]}]},{"cell_type":"code","source":["transcription_text = result[\"text\"]\n","\n","# Extract a segment\n","segment = transcription_text[:100]\n","print(\"\\nSample Segment:\")\n","print(segment)\n","\n","# Word count\n","word_count = len(segment.split())\n","print(f\"\\nWord Count of Segment: {word_count}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E3lwWepQg0ii","executionInfo":{"status":"ok","timestamp":1734689001595,"user_tz":-300,"elapsed":416,"user":{"displayName":"Dilrabo Khidirova","userId":"07641774209207942309"}},"outputId":"f780d215-cb86-41d0-d8ed-1d3af9f84d3d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Sample Segment:\n"," Good evening guys. Good evening. Hello. Did you choose Mr. Degis Alt? Not yet. I don't know. I chos\n","\n","Word Count of Segment: 19\n"]}]},{"cell_type":"code","source":["output_file = \"/content/transcription.txt\"\n","with open(output_file, \"w\") as f:\n","    f.write(transcription_text)\n","\n","print(f\"Transcription saved to {output_file}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pWIVKOY7hGvG","executionInfo":{"status":"ok","timestamp":1734689009350,"user_tz":-300,"elapsed":397,"user":{"displayName":"Dilrabo Khidirova","userId":"07641774209207942309"}},"outputId":"bceee901-c4dc-4d5f-87ac-f56b4327681f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Transcription saved to /content/transcription.txt\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0JbTuJ8sharm"},"execution_count":null,"outputs":[]}]}